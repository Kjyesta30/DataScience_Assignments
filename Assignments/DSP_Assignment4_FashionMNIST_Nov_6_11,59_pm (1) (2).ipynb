{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Dataset:\n",
    "\n",
    "Use fashion-mnist dataset for this assignment. You will have a chance to explore various Deep Learning models in this Assignment.\n",
    "\n",
    "# Dataset description:\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "# Labels:\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assignment 4 - Questions\n",
    "\n",
    "This Assignment focuses on exploring various Deep Learning models(neural networks) and model building. \n",
    "\n",
    "1. Load the fashion-mnist dataset from the tensorflow datasets or download from kaggle(https://www.kaggle.com/zalando-research/fashionmnist)\n",
    "2. Normalize the data - briefly comment why we need to normalize?\n",
    "3. Split the dataset into train(50,000), valid(10,000) and test sets(10,000)\n",
    "4. Reshape the input data to a 2D for MLP(Multi Layer perceptron) and tensor(4d) for CNN(Convolution Neural Network)\n",
    "5. Build a Neural Network Multi-Layer Perceptron Classifier model (you can use sklearn neural network MLP Classifier)\n",
    "6. Experiment with the architecture of the MLP classfier that you already built and report if you seen any improvement in the accuracy\n",
    "7. Build a basic sequential deep learning model(CNN) and compare the accuracy with MLP Classifier\n",
    "8. Experiment with the architecture of CNN and report if you see any improvement in the accuracy\n",
    "9. Evaluate MLPClassifier and CNN using F1 score values and accuracy(only for the best model)\n",
    "10. Explain the paramters to tune to reduce the risk of overfitting in deep learning models\n",
    "10. Optional Explain the meaning of Precision, Recall and F1-Score and why these are used to evaluate Classification models (instead of using Accuracy as a metric). \n",
    "\n",
    "10. optional: Try any advanced deep learning model\n",
    "\n",
    "Submit the .ipynb, and .html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all required libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense, Dropout, Activation,Flatten,MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load the fashion-mnist dataset from the tensorflow datasets or download from kaggle(https://www.kaggle.com/zalando-research/fashionmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data into dataframes\n",
    "data_train = pd.read_csv('fashion-mnist_train.csv')\n",
    "data_test = pd.read_csv('fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Normalize the data - briefly comment why we need to normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising data\n",
    "train_data = np.array(data_train, dtype='float32')\n",
    "test_data = np.array(data_test, dtype='float32')\n",
    "\n",
    "X_train = train_data[:, 1:] / 255\n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "X_test = test_data[:, 1:] / 255\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization is necessary to ensure that all the data are on the same scale and this usually improves the performance. For the Fashion MNIST\n",
    "data set, the normalization for training dataset and the validation data set is handled as mapping [0,255] to the [0,1] that will increase the training speed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Split the dataset into train(50,000), valid(10,000) and test sets(10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Reshape the input data to a 2D for MLP(Multi Layer perceptron) and tensor(4d) for CNN(Convolution Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data to 2D for MLP\n",
    "X_train = X_train.reshape(50000, 784)\n",
    "X_valid = X_valid.reshape(10000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Build a Neural Network Multi-Layer Perceptron Classifier model (you can use sklearn neural network MLP Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Experiment with the architecture of the MLP classfier that you already built and report if you seen any improvement in the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.31405484\n",
      "Iteration 2, loss = 0.85496585\n",
      "Iteration 3, loss = 0.76453662\n",
      "Iteration 4, loss = 0.70416721\n",
      "Iteration 5, loss = 0.66379934\n",
      "Iteration 6, loss = 0.63384224\n",
      "Iteration 7, loss = 0.60988214\n",
      "Iteration 8, loss = 0.59113659\n",
      "Iteration 9, loss = 0.57092746\n",
      "Iteration 10, loss = 0.55422922\n",
      "Iteration 11, loss = 0.53478312\n",
      "Iteration 12, loss = 0.52524976\n",
      "Iteration 13, loss = 0.50984523\n",
      "Iteration 14, loss = 0.50003477\n",
      "Iteration 15, loss = 0.49057679\n",
      "Iteration 16, loss = 0.47923097\n",
      "Iteration 17, loss = 0.46735221\n",
      "Iteration 18, loss = 0.46056224\n",
      "Iteration 19, loss = 0.44901337\n",
      "Iteration 20, loss = 0.44182715\n",
      "Iteration 21, loss = 0.43283593\n",
      "Iteration 22, loss = 0.42375932\n",
      "Iteration 23, loss = 0.41889549\n",
      "Iteration 24, loss = 0.40757107\n",
      "Iteration 25, loss = 0.39935114\n",
      "Iteration 26, loss = 0.39765259\n",
      "Iteration 27, loss = 0.38867228\n",
      "Iteration 28, loss = 0.38146999\n",
      "Iteration 29, loss = 0.37579527\n",
      "Iteration 30, loss = 0.36443001\n",
      "Iteration 31, loss = 0.36046088\n",
      "Iteration 32, loss = 0.35246170\n",
      "Iteration 33, loss = 0.34991160\n",
      "Iteration 34, loss = 0.34371355\n",
      "Iteration 35, loss = 0.33835984\n",
      "Iteration 36, loss = 0.33018969\n",
      "Iteration 37, loss = 0.32662693\n",
      "Iteration 38, loss = 0.32198643\n",
      "Iteration 39, loss = 0.31326890\n",
      "Iteration 40, loss = 0.31361721\n",
      "Iteration 41, loss = 0.30549527\n",
      "Iteration 42, loss = 0.30026009\n",
      "Iteration 43, loss = 0.30171248\n",
      "Iteration 44, loss = 0.29357019\n",
      "Iteration 45, loss = 0.28502275\n",
      "Iteration 46, loss = 0.28146531\n",
      "Iteration 47, loss = 0.28397066\n",
      "Iteration 48, loss = 0.27546100\n",
      "Iteration 49, loss = 0.27444651\n",
      "Iteration 50, loss = 0.26783905\n",
      "Iteration 51, loss = 0.26088499\n",
      "Iteration 52, loss = 0.25642961\n",
      "Iteration 53, loss = 0.25450111\n",
      "Iteration 54, loss = 0.24934290\n",
      "Iteration 55, loss = 0.24625378\n",
      "Iteration 56, loss = 0.24321028\n",
      "Iteration 57, loss = 0.24132357\n",
      "Iteration 58, loss = 0.23378961\n",
      "Iteration 59, loss = 0.23405428\n",
      "Iteration 60, loss = 0.23067411\n",
      "Iteration 61, loss = 0.22560144\n",
      "Iteration 62, loss = 0.22067732\n",
      "Iteration 63, loss = 0.21716386\n",
      "Iteration 64, loss = 0.21727736\n",
      "Iteration 65, loss = 0.21511284\n",
      "Iteration 66, loss = 0.21085383\n",
      "Iteration 67, loss = 0.20615498\n",
      "Iteration 68, loss = 0.20360212\n",
      "Iteration 69, loss = 0.20164930\n",
      "Iteration 70, loss = 0.19803225\n",
      "Iteration 71, loss = 0.19677992\n",
      "Iteration 72, loss = 0.19045348\n",
      "Iteration 73, loss = 0.18814903\n",
      "Iteration 74, loss = 0.19188468\n",
      "Iteration 75, loss = 0.18300244\n",
      "Iteration 76, loss = 0.18441354\n",
      "Iteration 77, loss = 0.17533584\n",
      "Iteration 78, loss = 0.17672859\n",
      "Iteration 79, loss = 0.17845615\n",
      "Iteration 80, loss = 0.17170155\n",
      "Iteration 81, loss = 0.17077074\n",
      "Iteration 82, loss = 0.16776272\n",
      "Iteration 83, loss = 0.16320091\n",
      "Iteration 84, loss = 0.16361637\n",
      "Iteration 85, loss = 0.16205016\n",
      "Iteration 86, loss = 0.16368585\n",
      "Iteration 87, loss = 0.15816814\n",
      "Iteration 88, loss = 0.15567817\n",
      "Iteration 89, loss = 0.14882618\n",
      "Iteration 90, loss = 0.15053766\n",
      "Iteration 91, loss = 0.15017661\n",
      "Iteration 92, loss = 0.14527288\n",
      "Iteration 93, loss = 0.14214156\n",
      "Iteration 94, loss = 0.13997608\n",
      "Iteration 95, loss = 0.13778444\n",
      "Iteration 96, loss = 0.13396924\n",
      "Iteration 97, loss = 0.14258972\n",
      "Iteration 98, loss = 0.13552353\n",
      "Iteration 99, loss = 0.13592057\n",
      "Iteration 100, loss = 0.13058958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnajyesta/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(150,), max_iter=100, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building MLP Classifier\n",
    "mlp_clf = MLPClassifier(max_iter=100, verbose=True,\n",
    "                        hidden_layer_sizes=(150,))\n",
    "\n",
    "mlp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "mlp1_pred = mlp_clf.predict(X_test)\n",
    "mlp1_pred = np.argmax(mlp1_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "mlp1_f1 = metrics.f1_score(y_true, mlp1_pred, average= \"weighted\")\n",
    "mlp1_accuracy = metrics.accuracy_score(y_true, mlp1_pred)\n",
    "mlp1_confusion_matrix = metrics.confusion_matrix(y_true, mlp1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------1st MLP Classifier Report---------------\n",
      "F1 score: 0.8760185150665216\n",
      "Accuracy score: 0.8742\n",
      "Confusion matrix: \n",
      " [[871   0   8  16   1   1 101   0   2   0]\n",
      " [  7 983   0   6   0   1   2   0   1   0]\n",
      " [ 86   0 780  11  52   0  69   0   2   0]\n",
      " [ 56   9  11 905  11   0   8   0   0   0]\n",
      " [121   1  52  50 726   1  48   0   1   0]\n",
      " [ 16   1   0   1   0 946   1  25   0  10]\n",
      " [193   1  56  28  23   0 692   0   7   0]\n",
      " [ 11   0   0   0   0  22   0 938   0  29]\n",
      " [ 27   0   3   4   2   2  14   1 947   0]\n",
      " [ 11   0   0   0   0   6   0  28   1 954]]\n"
     ]
    }
   ],
   "source": [
    "#Printing evaluation metrics for MLP Classifier\n",
    "print(\"-----------------1st MLP Classifier Report---------------\")\n",
    "print(\"F1 score: {}\".format(mlp1_f1))\n",
    "print(\"Accuracy score: {}\".format(mlp1_accuracy))\n",
    "print(\"Confusion matrix: \\n\", mlp1_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnajyesta/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 35, 10), max_iter=100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,35,10),\n",
    "                       max_iter=100,\n",
    "                       activation = 'tanh')\n",
    "mlp2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "mlp2_pred = mlp2.predict(X_test)\n",
    "mlp2_pred = np.argmax(mlp2_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "mlp2_f1 = metrics.f1_score(y_true, mlp2_pred, average= \"weighted\")\n",
    "mlp2_accuracy = metrics.accuracy_score(y_true, mlp2_pred)\n",
    "mlp2_confusion_matrix = metrics.confusion_matrix(y_true, mlp2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------2nd MLP Classifier Report---------------\n",
      "F1 score: 0.8784498038942871\n",
      "Accuracy score: 0.8772\n",
      "Confusion matrix: \n",
      " [[825   3   7  17   2   0 140   0   6   0]\n",
      " [  7 977   0  10   0   1   5   0   0   0]\n",
      " [ 79   1 717  15  83   0 104   0   1   0]\n",
      " [ 52  12   4 887  27   0  16   0   2   0]\n",
      " [ 29   0  47  23 829   0  69   0   3   0]\n",
      " [ 15   1   1   1   0 937   0  31   1  13]\n",
      " [140   2  29  29  48   0 746   0   6   0]\n",
      " [ 13   0   0   0   0  17   0 932   0  38]\n",
      " [ 13   1   4   1   2   1  12   2 962   2]\n",
      " [  6   0   0   0   0   5   1  26   2 960]]\n"
     ]
    }
   ],
   "source": [
    "#Printing evaluation metrics for MLP Classifier\n",
    "print(\"-----------------2nd MLP Classifier Report---------------\")\n",
    "print(\"F1 score: {}\".format(mlp2_f1))\n",
    "print(\"Accuracy score: {}\".format(mlp2_accuracy))\n",
    "print(\"Confusion matrix: \\n\", mlp2_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above both MLP Classifier,we can see there is improvement in performance and we can say that the second MLP classifier has high F1 Score and high accuracy and it slightly performed better than the first MLP Classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Build a basic sequential deep learning model(CNN) and compare the accuracy with MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data to 4D\n",
    "X_train = X_train.reshape([-1, 28, 28, 1])\n",
    "X_test = X_test.reshape([-1, 28, 28, 1])\n",
    "X_valid=X_valid.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building CNN Model\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "modelCNN.add(layers.MaxPooling2D((2, 2)))\n",
    "modelCNN.add(layers.Flatten())\n",
    "modelCNN.add(layers.Dense(100, activation='tanh', kernel_initializer='he_uniform'))\n",
    "modelCNN.add(layers.Dense(50, activation='tanh', kernel_initializer='he_uniform'))\n",
    "modelCNN.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "modelCNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5292 - accuracy: 0.8231 - val_loss: 0.3871 - val_accuracy: 0.8674\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3563 - accuracy: 0.8740 - val_loss: 0.3361 - val_accuracy: 0.8837\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3046 - accuracy: 0.8919 - val_loss: 0.3124 - val_accuracy: 0.8907\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2745 - accuracy: 0.9019 - val_loss: 0.2906 - val_accuracy: 0.8978\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2519 - accuracy: 0.9114 - val_loss: 0.2720 - val_accuracy: 0.9026\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2310 - accuracy: 0.9178 - val_loss: 0.2651 - val_accuracy: 0.9067\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2129 - accuracy: 0.9245 - val_loss: 0.2542 - val_accuracy: 0.9085\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1983 - accuracy: 0.9303 - val_loss: 0.2470 - val_accuracy: 0.9123\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1841 - accuracy: 0.9362 - val_loss: 0.2530 - val_accuracy: 0.9095\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1716 - accuracy: 0.9398 - val_loss: 0.2629 - val_accuracy: 0.9051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa372820430>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating CNN Model\n",
    "cnn_pred = modelCNN.predict(X_test)\n",
    "cnn_pred = np.argmax(cnn_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "cnn_f1 = metrics.f1_score(y_true, cnn_pred, average= \"weighted\")\n",
    "cnn_accuracy = metrics.accuracy_score(y_true, cnn_pred)\n",
    "cnn_confusion_matrix = metrics.confusion_matrix(y_true, cnn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------1st Convolutional Neural Network Report---------------\n",
      "F1 score: 0.9083799513240239\n",
      "Accuracy score: 0.9086\n",
      "Confusion matrix: \n",
      " [[812   0  13  30   0   1 136   0   8   0]\n",
      " [  0 986   1  10   0   0   3   0   0   0]\n",
      " [ 14   1 889  20  21   0  52   0   3   0]\n",
      " [  8   9   2 973   3   0   5   0   0   0]\n",
      " [  2   2  94  59 771   0  72   0   0   0]\n",
      " [  0   1   0   0   0 965   0  24   2   8]\n",
      " [ 74   2  63  48  26   0 781   0   6   0]\n",
      " [  0   0   0   0   0   3   0 972   0  25]\n",
      " [  4   2   4   2   1   4   6   2 973   2]\n",
      " [  0   0   0   0   0   3   0  33   0 964]]\n"
     ]
    }
   ],
   "source": [
    "# Printing evaluating metrics for CNN Model\n",
    "print(\"-----------------1st Convolutional Neural Network Report---------------\")\n",
    "print(\"F1 score: {}\".format(cnn_f1))\n",
    "print(\"Accuracy score: {}\".format(cnn_accuracy))\n",
    "print(\"Confusion matrix: \\n\", cnn_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the accuracy with MLP Classifier, we can say that the CNN Model performed better than the MLP classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Experiment with the architecture of CNN and report if you see any improvement in the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with CNN Model\n",
    "modelCNN2 = Sequential()\n",
    "modelCNN2.add(layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "modelCNN2.add(layers.MaxPooling2D((2, 2)))\n",
    "modelCNN2.add(layers.Flatten())\n",
    "modelCNN2.add(layers.Dense(784, activation='tanh', kernel_initializer='he_uniform'))\n",
    "modelCNN2.add(Dropout(rate=0.3))\n",
    "modelCNN2.add(layers.Dense(500, activation='tanh', kernel_initializer='he_uniform'))\n",
    "modelCNN2.add(Dropout(rate=0.2))\n",
    "modelCNN2.add(layers.Dense(100, activation='tanh', kernel_initializer='he_uniform'))\n",
    "modelCNN2.add(Dropout(rate=0.1))\n",
    "modelCNN2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "modelCNN2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5711 - accuracy: 0.7886 - val_loss: 0.4171 - val_accuracy: 0.8447\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3895 - accuracy: 0.8591 - val_loss: 0.3499 - val_accuracy: 0.8735\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3485 - accuracy: 0.8745 - val_loss: 0.3327 - val_accuracy: 0.8839\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.3211 - accuracy: 0.8823 - val_loss: 0.3250 - val_accuracy: 0.8832\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.3036 - accuracy: 0.8889 - val_loss: 0.3105 - val_accuracy: 0.8903\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 36s 28ms/step - loss: 0.2853 - accuracy: 0.8964 - val_loss: 0.3165 - val_accuracy: 0.8894\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.2715 - accuracy: 0.9014 - val_loss: 0.2964 - val_accuracy: 0.8981\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.2626 - accuracy: 0.9051 - val_loss: 0.2931 - val_accuracy: 0.8984\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.2531 - accuracy: 0.9065 - val_loss: 0.2776 - val_accuracy: 0.9030\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 36s 28ms/step - loss: 0.2451 - accuracy: 0.9101 - val_loss: 0.2968 - val_accuracy: 0.8943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2eb78d9d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating CNN Model\n",
    "cnn2_pred = modelCNN2.predict(X_test)\n",
    "cnn2_pred = np.argmax(cnn2_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "cnn2_f1 = metrics.f1_score(y_true, cnn2_pred, average= \"weighted\")\n",
    "cnn2_accuracy = metrics.accuracy_score(y_true, cnn2_pred)\n",
    "cnn2_confusion_matrix = metrics.confusion_matrix(y_true, cnn2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------2nd Convolutional Neural Network Report---------------\n",
      "F1 score: 0.8966773985708857\n",
      "Accuracy score: 0.8964\n",
      "Confusion matrix: \n",
      " [[749   2  11  49   2   0 176   0  11   0]\n",
      " [  1 974   1  14   2   1   6   0   1   0]\n",
      " [  9   0 777  12 109   0  91   0   2   0]\n",
      " [  9   7   5 933  25   0  21   0   0   0]\n",
      " [  0   1  18  25 908   0  47   0   1   0]\n",
      " [  0   1   0   0   0 959   0  23   3  14]\n",
      " [ 61   2  30  36  68   0 793   0  10   0]\n",
      " [  0   0   0   0   0  14   0 927   0  59]\n",
      " [  2   2   3   3   3   3   8   3 971   2]\n",
      " [  0   0   0   0   0   3   0  24   0 973]]\n"
     ]
    }
   ],
   "source": [
    "#Printing metrics of CNN Model\n",
    "print(\"-----------------2nd Convolutional Neural Network Report---------------\")\n",
    "print(\"F1 score: {}\".format(cnn2_f1))\n",
    "print(\"Accuracy score: {}\".format(cnn2_accuracy))\n",
    "print(\"Confusion matrix: \\n\", cnn2_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluate MLPClassifier and CNN using F1 score values and accuracy(only for the best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------1st Convolutional Neural Network Report---------------\n",
      "F1 score: 0.9083799513240239\n",
      "Accuracy score: 0.9086\n",
      "Confusion matrix: \n",
      " [[812   0  13  30   0   1 136   0   8   0]\n",
      " [  0 986   1  10   0   0   3   0   0   0]\n",
      " [ 14   1 889  20  21   0  52   0   3   0]\n",
      " [  8   9   2 973   3   0   5   0   0   0]\n",
      " [  2   2  94  59 771   0  72   0   0   0]\n",
      " [  0   1   0   0   0 965   0  24   2   8]\n",
      " [ 74   2  63  48  26   0 781   0   6   0]\n",
      " [  0   0   0   0   0   3   0 972   0  25]\n",
      " [  4   2   4   2   1   4   6   2 973   2]\n",
      " [  0   0   0   0   0   3   0  33   0 964]]\n"
     ]
    }
   ],
   "source": [
    "#Printing metrics of Best CNN Model\n",
    "print(\"-----------------1st Convolutional Neural Network Report---------------\")\n",
    "print(\"F1 score: {}\".format(cnn_f1))\n",
    "print(\"Accuracy score: {}\".format(cnn_accuracy))\n",
    "print(\"Confusion matrix: \\n\", cnn_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------2nd MLP Classifier Report---------------\n",
      "F1 score: 0.8784498038942871\n",
      "Accuracy score: 0.8772\n",
      "Confusion matrix: \n",
      " [[825   3   7  17   2   0 140   0   6   0]\n",
      " [  7 977   0  10   0   1   5   0   0   0]\n",
      " [ 79   1 717  15  83   0 104   0   1   0]\n",
      " [ 52  12   4 887  27   0  16   0   2   0]\n",
      " [ 29   0  47  23 829   0  69   0   3   0]\n",
      " [ 15   1   1   1   0 937   0  31   1  13]\n",
      " [140   2  29  29  48   0 746   0   6   0]\n",
      " [ 13   0   0   0   0  17   0 932   0  38]\n",
      " [ 13   1   4   1   2   1  12   2 962   2]\n",
      " [  6   0   0   0   0   5   1  26   2 960]]\n"
     ]
    }
   ],
   "source": [
    "# Printing Metrics for best MLP Model\n",
    "print(\"-----------------2nd MLP Classifier Report---------------\")\n",
    "print(\"F1 score: {}\".format(mlp2_f1))\n",
    "print(\"Accuracy score: {}\".format(mlp2_accuracy))\n",
    "print(\"Confusion matrix: \\n\", mlp2_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above is the evaluation metrics i.e. F1 Score,Accuracy Score and confusion matrix for the best model.**\n",
    "- We can see that CNN Model performed better among all the models above with 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Explain the paramters to tune to reduce the risk of overfitting in deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters to tune to reduce the risk of overfitting in deep learning models:**  \n",
    "\n",
    "  1) Number of epochs will affect the performance. For large number of epochs , there is improvement in performance. \n",
    "    But need to do certain experimentation for deciding epochs, learning rate. We can see after certain epochs there \n",
    "    is not any reduction is training loss and improvement in training accuracy. Accordingly we can decide number of \n",
    "    epochs. Also we can use dropout layer in the CNN model. We can use various optimizers e.g SGD,rmsprop,Adam etc.\n",
    "    \n",
    "  2) Learning rate controls how quickly or slowly a model learns a problem and tuning learning rate provides \n",
    "    perhaps the most important hyperparameter to tune for your model in order to achieve good performance \n",
    "    on your problem. A common way is to make the initial learning rate 10 times smaller than the \n",
    "    one used for intial training.\n",
    "    \n",
    "  3)Batch size - Batch size has an effect on the resource requirements of the training process, speed and \n",
    "    number of iterations.Small batch sizes add regularization while large batch sizes add less, so we use this while \n",
    "    balancing the proper amount of regularization.\n",
    "    \n",
    "  4)Number of hidden units - The number of hidden units is the measure of modelâ€™s learning capacity.Slightly more \n",
    "    number of units then optimal number is not a problem,but a much larger number will lead to overfitting.\n",
    "    \n",
    "  5)Number of layers - Increasing the number of hidden units and/or layers may lead to overfitting because it \n",
    "    will make it easier for the neural network to memorize the training set, that is to learn a function \n",
    "    that perfectly separates the training set but that does not generalize to unseen data.\n",
    "    \n",
    "All these things will help to improve the performance of deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
